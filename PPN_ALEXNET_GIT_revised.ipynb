{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring torch variables\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "best_acc = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def alexnet(**kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    \"\"\"\n",
    "    model = AlexNet(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceAttack_HZ(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        self.num_classes=num_classes\n",
    "        super(InferenceAttack_HZ, self).__init__()\n",
    "        self.features=nn.Sequential(\n",
    "            nn.Linear(100,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.labels=nn.Sequential(\n",
    "           nn.Linear(num_classes,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.combine=nn.Sequential(\n",
    "            nn.Linear(64*2,256),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1),\n",
    "            )\n",
    "        for key in self.state_dict():\n",
    "            print (key)\n",
    "            if key.split('.')[-1] == 'weight':    \n",
    "                nn.init.normal(self.state_dict()[key], std=0.01)\n",
    "                print (key)\n",
    "                \n",
    "            elif key.split('.')[-1] == 'bias':\n",
    "                self.state_dict()[key][...] = 0\n",
    "        self.output= nn.Sigmoid()\n",
    "    def forward(self,x,l):\n",
    "        \n",
    "        out_x = self.features(x)\n",
    "        out_l = self.labels(l)\n",
    "        \n",
    "        is_member =self.combine( torch.cat((out_x  ,out_l),1))\n",
    "        \n",
    "        return self.output(is_member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='cifar100'\n",
    "checkpoint_path='/home/ymohit/Robust-MIA/logs/milad/checkpoint/checkpoints_100cifar_alexnet_white'\n",
    "train_batch=100\n",
    "test_batch=100\n",
    "lr=0.05\n",
    "epochs=500\n",
    "state={}\n",
    "state['lr']=lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_privatly(trainloader, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=10000,alpha=0.9):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    inference_model.eval()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "\n",
    "    first_id = -1\n",
    "    for batch_idx, (inputs, targets) in (trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((outputs.size(0),100))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, targets.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "        inference_output = inference_model ( outputs,infer_input_one_hot)\n",
    "        #print (inference_output.mean())\n",
    "        \n",
    "\n",
    "        loss = criterion(outputs, targets) + (alpha)*(((inference_output-1.0).pow(2).mean()))\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "#         top1.update(prec1[0], inputs.size(0))\n",
    "#         top5.update(prec5[0], inputs.size(0))\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=500,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "        if batch_idx-first_id >= num_batchs:\n",
    "            break\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "#         top1.update(prec1[0], inputs.size(0))\n",
    "#         top5.update(prec5[0], inputs.size(0))\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%100==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    ))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "#         losses.update(loss.data[0], inputs.size(0))\n",
    "#         top1.update(prec1[0], inputs.size(0))\n",
    "#         top5.update(prec5[0], inputs.size(0))\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx % 100==0:\n",
    "            \n",
    "            print ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(testloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        ))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def privacy_train(trainloader, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    \n",
    "    inference_model.train()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "\n",
    "    end = time.time()\n",
    "    first_id = -1\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "#         mtop1_a.update(mtop1[0], model_input.size(0))\n",
    "#         mtop5_a.update(mtop5[0], model_input.size(0))\n",
    "\n",
    "                \n",
    "        mtop1_a.update(mtop1.item(), model_input.size(0))\n",
    "        mtop5_a.update(mtop5.item(), model_input.size(0))\n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),100))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "\n",
    "        attack_model_input = pred_outputs#torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "        \n",
    "        \n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        #losses.update(loss.data[0], model_input.size(0))\n",
    "        \n",
    "        losses.update(loss.item(), model_input.size(0))\n",
    "        \n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id > num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "        if batch_idx%10==0:\n",
    "            print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx ,\n",
    "                    size=500,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    ))\n",
    "\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def privacy_test(trainloader, model,inference_model, criterion, optimizer, epoch, use_cuda,num_batchs=1000):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    mtop1_a = AverageMeter()\n",
    "    mtop5_a = AverageMeter()\n",
    "    \n",
    "    inference_model.eval()\n",
    "    model.eval()\n",
    "    # switch to evaluate mode\n",
    "\n",
    "    end = time.time()\n",
    "    for batch_idx,((tr_input, tr_target) ,(te_input, te_target)) in trainloader:\n",
    "        # measure data loading time\n",
    "        if first_id == -1:\n",
    "            first_id = batch_idx\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "        tr_input = tr_input.cuda()\n",
    "        te_input = te_input.cuda()\n",
    "        tr_target = tr_target.cuda()\n",
    "        te_target = te_target.cuda()\n",
    "        \n",
    "        \n",
    "        v_tr_input = torch.autograd.Variable(tr_input)\n",
    "        v_te_input = torch.autograd.Variable(te_input)\n",
    "        v_tr_target = torch.autograd.Variable(tr_target)\n",
    "        v_te_target = torch.autograd.Variable(te_target)\n",
    "        \n",
    "        # compute output\n",
    "        model_input =torch.cat((v_tr_input,v_te_input))\n",
    "        \n",
    "        pred_outputs = model(model_input)\n",
    "        \n",
    "        infer_input= torch.cat((v_tr_target,v_te_target))\n",
    "        \n",
    "        mtop1, mtop5 =accuracy(pred_outputs.data, infer_input.data, topk=(1, 5))\n",
    "        \n",
    "#         mtop1_a.update(mtop1[0], model_input.size(0))\n",
    "#         mtop5_a.update(mtop5[0], model_input.size(0))\n",
    "\n",
    "        mtop1_a.update(mtop1.item(), model_input.size(0))\n",
    "        mtop5_a.update(mtop5.item(), model_input.size(0))\n",
    "        \n",
    "        \n",
    "        one_hot_tr = torch.from_numpy((np.zeros((infer_input.size(0),100))-1)).cuda().type(torch.cuda.FloatTensor)\n",
    "        target_one_hot_tr = one_hot_tr.scatter_(1, infer_input.type(torch.cuda.LongTensor).view([-1,1]).data,1)\n",
    "\n",
    "        infer_input_one_hot = torch.autograd.Variable(target_one_hot_tr)\n",
    "        \n",
    "\n",
    "        attack_model_input = pred_outputs#torch.cat((pred_outputs,infer_input_one_hot),1)\n",
    "        member_output = inference_model(attack_model_input,infer_input_one_hot)\n",
    "        \n",
    "        \n",
    "        \n",
    "        is_member_labels = torch.from_numpy(np.reshape(np.concatenate((np.zeros(v_tr_input.size(0)),np.ones(v_te_input.size(0)))),[-1,1])).cuda()\n",
    "        \n",
    "        v_is_member_labels = torch.autograd.Variable(is_member_labels).type(torch.cuda.FloatTensor)\n",
    "\n",
    "        loss = criterion(member_output, v_is_member_labels)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1=np.mean((member_output.data.cpu().numpy() >0.5)==v_is_member_labels.data.cpu().numpy())\n",
    "        #losses.update(loss.data[0], model_input.size(0))\n",
    "        \n",
    "        losses.update(loss.item(), model_input.size(0))\n",
    "        \n",
    "        top1.update(prec1, model_input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if batch_idx-first_id >= num_batchs:\n",
    "            break\n",
    "\n",
    "        # plot progress\n",
    "#         if batch_idx%10==0:\n",
    "#             print  ('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "#                     batch=batch_idx ,\n",
    "#                     size=len(trainloader),\n",
    "#                     data=data_time.avg,\n",
    "#                     bt=batch_time.avg,\n",
    "#                     loss=losses.avg,\n",
    "#                     top1=top1.avg,\n",
    "#                     ))\n",
    "\n",
    "    return (losses.avg, top1.avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in [20,40]:\n",
    "        state['lr'] *= 0.1 \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']\n",
    "\n",
    "\n",
    "def save_checkpoint_adversary(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_adversary_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "global best_acc\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    mkdir_p(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cifar100\n",
      "==> creating model \n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing dataset %s' % dataset)\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        #transforms.RandomCrop(32, padding=4),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    \n",
    "    \n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "# prepare test data parts\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "    \n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    dataloader = datasets.CIFAR10\n",
    "    num_classes = 10\n",
    "else:\n",
    "    dataloader = datasets.CIFAR100\n",
    "    num_classes = 100\n",
    "\n",
    "\n",
    "\n",
    "# Model\n",
    "print(\"==> creating model \")\n",
    "\n",
    "\n",
    "model = AlexNet(num_classes)\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 2.50M\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_attack = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.weight\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "features.2.weight\n",
      "features.2.bias\n",
      "features.4.weight\n",
      "features.4.weight\n",
      "features.4.bias\n",
      "labels.0.weight\n",
      "labels.0.weight\n",
      "labels.0.bias\n",
      "labels.2.weight\n",
      "labels.2.weight\n",
      "labels.2.bias\n",
      "combine.0.weight\n",
      "combine.0.weight\n",
      "combine.0.bias\n",
      "combine.2.weight\n",
      "combine.2.weight\n",
      "combine.2.bias\n",
      "combine.4.weight\n",
      "combine.4.weight\n",
      "combine.4.bias\n",
      "combine.6.weight\n",
      "combine.6.weight\n",
      "combine.6.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymohit/anaconda3/envs/rmia/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    }
   ],
   "source": [
    "# Resume\n",
    "title = 'cifar-100'\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "inferenece_model = InferenceAttack_HZ(100).cuda()\n",
    "private_train_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "optimizer_mem = optim.Adam(inferenece_model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_100_dir = '/home/ymohit/Robust-MIA/logs/milad/data100'\n",
    "\n",
    "\n",
    "batch_privacy=100\n",
    "trainset = dataloader(root=data_100_dir, train=True, download=True, transform=transform_train)\n",
    "trainloader = data.DataLoader(trainset, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "trainset_private = dataloader(root=data_100_dir, train=True, download=True, transform=transform_test)\n",
    "trainloader_private = data.DataLoader(trainset, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "testset = dataloader(root=data_100_dir, train=False, download=False, transform=transform_test)\n",
    "testloader = data.DataLoader(testset, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_privacy=100\n",
    "trainset = dataloader(root=data_100_dir, train=True, download=True, transform=transform_test)\n",
    "testset = dataloader(root=data_100_dir, train=False, download=False, transform=transform_test)\n",
    "\n",
    "r = np.arange(50000)\n",
    "np.random.shuffle(r)\n",
    "\n",
    "private_trainset_intrain = []\n",
    "private_trainset_intest = []\n",
    "\n",
    "private_testset_intrain =[] \n",
    "private_testset_intest =[] \n",
    "\n",
    "\n",
    "for i in range(25000):\n",
    "    private_trainset_intrain.append(trainset[r[i]])\n",
    "\n",
    "for i in range(25000,50000):\n",
    "    private_testset_intrain.append(trainset[r[i]])\n",
    "\n",
    "    \n",
    "r = np.arange(10000)\n",
    "np.random.shuffle(r)\n",
    "  \n",
    "for i in range(5000):\n",
    "    private_trainset_intest.append(testset[r[i]])\n",
    "\n",
    "\n",
    "for i in range(5000,10000):\n",
    "    private_testset_intest.append(testset[r[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_trainloader_intrain = data.DataLoader(private_trainset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_trainloader_intest = data.DataLoader(private_trainset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "private_testloader_intrain = data.DataLoader(private_testset_intrain, batch_size=batch_privacy, shuffle=True, num_workers=1)\n",
    "private_testloader_intest = data.DataLoader(private_testset_intest, batch_size=batch_privacy, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 500] LR: 0.050000\n",
      "(1/500) Data: 0.003s | Batch: 0.691s | | Loss: 4.6006 | top1:  1.0000 | top5:  10.0000\n",
      "(101/500) Data: 0.016s | Batch: 0.035s | | Loss: 4.5949 | top1:  1.2079 | top5:  5.7624\n",
      "(201/500) Data: 0.015s | Batch: 0.030s | | Loss: 4.4762 | top1:  2.1294 | top5:  9.7413\n",
      "(301/500) Data: 0.015s | Batch: 0.028s | | Loss: 4.3641 | top1:  3.0532 | top5:  13.1661\n",
      "(401/500) Data: 0.015s | Batch: 0.028s | | Loss: 4.2718 | top1:  4.0673 | top5:  16.1746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymohit/anaconda3/envs/rmia/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/100) Data: 0.166s | Batch: 0.175s | Loss: 3.9642 | top1:  11.0000 | top5:  27.0000\n",
      "test acc 8.99\n",
      "\n",
      "Epoch: [2 | 500] LR: 0.050000\n",
      "(1/500) Data: 0.003s | Batch: 0.019s | | Loss: 3.7881 | top1:  9.0000 | top5:  33.0000\n",
      "(101/500) Data: 0.013s | Batch: 0.025s | | Loss: 3.7824 | top1:  9.7129 | top5:  32.9802\n",
      "(201/500) Data: 0.013s | Batch: 0.024s | | Loss: 3.7663 | top1:  10.9453 | top5:  33.6965\n",
      "(301/500) Data: 0.013s | Batch: 0.023s | | Loss: 3.7294 | top1:  11.6013 | top5:  34.6146\n",
      "(401/500) Data: 0.013s | Batch: 0.023s | | Loss: 3.6984 | top1:  12.1022 | top5:  35.6983\n",
      "(1/100) Data: 0.153s | Batch: 0.159s | Loss: 3.6862 | top1:  15.0000 | top5:  35.0000\n",
      "test acc 14.79\n",
      "\n",
      "Epoch: [3 | 500] LR: 0.050000\n",
      "(1/500) Data: 0.003s | Batch: 0.022s | | Loss: 3.2347 | top1:  17.0000 | top5:  49.0000\n"
     ]
    }
   ],
   "source": [
    "is_best=False\n",
    "best_acc=0.0\n",
    "start_epoch=0\n",
    "# Train and val\n",
    "for epoch in range(start_epoch, 400):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "\n",
    "    train_enum = enumerate(trainloader)\n",
    "    train_private_enum = enumerate(zip(trainloader_private,testloader))\n",
    "    for i in range(500//2):\n",
    "        \n",
    "        \n",
    "        if epoch>3:\n",
    "            privacy_loss, privacy_acc = privacy_train(train_private_enum,model,inferenece_model,criterion_attack,optimizer_mem,epoch,use_cuda,1)\n",
    "            train_loss, train_acc = train_privatly(train_enum, model,inferenece_model, criterion, optimizer, epoch, use_cuda,1,1)\n",
    "            \n",
    "            \n",
    "            if i%10 ==0:\n",
    "                print ('privacy res',privacy_acc,train_acc)\n",
    "            if  (i+1)%50 ==0:\n",
    "                train_private_enum = enumerate(zip(trainloader_private,testloader))\n",
    "        else:\n",
    "            train_loss, train_acc = train_privatly(train_enum, model,inferenece_model, criterion, optimizer, epoch, use_cuda,1000,0)\n",
    "            break\n",
    "        \n",
    "        \n",
    "    test_loss, test_acc = test(testloader, model, criterion, epoch, use_cuda)\n",
    "\n",
    "    print ('test acc',test_acc)\n",
    "\n",
    "    # save model\n",
    "    is_best = test_acc>best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'acc': test_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, False, checkpoint=checkpoint_path,filename='epoch%d'%epoch)\n",
    "\n",
    "print('Best acc:')\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
